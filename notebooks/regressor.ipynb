{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad942dbb",
   "metadata": {},
   "source": [
    "# Machine Learning (Scikit-learn, XGBoost, LightGBM, CatBoost): Regressor\n",
    "\n",
    "In this document, the performance of 8 different Machine Learning (ML) algorithms are\n",
    "compared to solve the regression problem, this is, to predict the value of a continuous variable, in this case, it is the fuel consumption of a car. The following algorithms are implemented:\n",
    "\n",
    "Algorithm 1: Lasso (Linear regressions with L1 regularization).\n",
    "\n",
    "Algorithm 2: Ridge (Linear regressions with L2 regularization).\n",
    "\n",
    "Algorithm 3: DecisionTreeRegressor.\n",
    "\n",
    "Algorithm 4: RandomForestRegressor.\n",
    "\n",
    "Algorithm 5: GradientBoostingRegressor.\n",
    "\n",
    "Algorithm 6: XGBRegressor (XGBoost library).\n",
    "\n",
    "Algorithm 7: LGBMRegressor (LightGBM library).\n",
    "\n",
    "Algorithm 8: CatBoostRegressor (CatBoost library)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae5ad2b",
   "metadata": {},
   "source": [
    "## Exploratory data analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22212815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import joblib\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from lightgbm import LGBMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec65717f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read DataFrame\n",
    "df = pd.read_csv('../data/raw/auto_cons_us.csv', sep=',', header=0)\n",
    "\n",
    "# Show DataFrame\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e901b4f",
   "metadata": {},
   "source": [
    "Column 'Fuel consumption' will be renamed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe37426d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename column\n",
    "df = df.rename(columns={'Fuel consumption': 'target'})\n",
    "\n",
    "# DataFrame information\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a393486",
   "metadata": {},
   "source": [
    "Format of all columns is correct.\n",
    "\n",
    "## Data preprocessing\n",
    "Data preprocessing consist of:\n",
    "1. Filling null values and dropping duplicates.\n",
    "2. Processing outliers and multicollinearity.\n",
    "3. Converting categorical variables into binary ones.\n",
    "4. Standardizing (scaling) the data.\n",
    "\n",
    "### Fill null values and drop duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3f637e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the number of null values per column\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bb4246",
   "metadata": {},
   "source": [
    "There are a few null values, they can be deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b93649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete null values\n",
    "df = df.dropna()\n",
    "\n",
    "# Verify the number of null values per column\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32441831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete duplicate rows\n",
    "df = df.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Show duplicate rows\n",
    "print(df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2b0d94",
   "metadata": {},
   "source": [
    "### Process outliers and multicollinearity\n",
    "A box plot is shown to verify if there exist outliers (no considerable outliers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4feff66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plot\n",
    "df.drop('target',axis=1).plot(kind='box', figsize=[8,4],\n",
    "title='Distribution of numeric features', xlabel='Features', ylabel='Value')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22b5391",
   "metadata": {},
   "source": [
    "In order to see multicollinearity, a heatmap is shown (excluding column 'Origin')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e0e1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "cm = df.drop('Origin',axis=1).corr()\n",
    "\n",
    "# Heatmap\n",
    "plt.figure(figsize = (12,12))\n",
    "sns.heatmap(cm, annot=True, square=True, cmap='coolwarm', fmt='.3f',\n",
    "annot_kws={\"size\": 10}, linewidths=0.5, linecolor='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584be6af",
   "metadata": {},
   "source": [
    "Column 'target' is correlated to all columns and '# of cylinders' is correlated to\t'Engine displacement'.\n",
    "\n",
    "**Remark:** Processed data is now saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e93478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed data\n",
    "df.to_csv('../data/processed/auto_cons_us_processed.csv', index=False)\n",
    "\n",
    "# DataFrame information\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f70b096",
   "metadata": {},
   "source": [
    "### Convert categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0935c2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the columns of the DataFrame\n",
    "print(df.columns)\n",
    "\n",
    "# Convert categorical variables to dummy variables\n",
    "df = pd.get_dummies(df)\n",
    "print(df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb71579",
   "metadata": {},
   "source": [
    "Two columns were added.\n",
    "\n",
    "### Data standarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da1a101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain characteristic matrix (x) and objective variable (y)\n",
    "x = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "x, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Train the StandardScaler by using 'x_train', then transform 'x_train' and 'x_test'\n",
    "scaler = StandardScaler()\n",
    "x_train_st = scaler.fit_transform(x_train)\n",
    "x_test_st = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fa1747",
   "metadata": {},
   "source": [
    "## Models belonging to Scikit-Learn library\n",
    "Five different models will be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebe5d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the regression models\n",
    "models = [Lasso(), Ridge(), DecisionTreeRegressor(),\n",
    "          RandomForestRegressor(), GradientBoostingRegressor()]\n",
    "\n",
    "def mape(y_true, y_pred):\n",
    "    '''Function to calculate Mean Absolute Percentage Error (MAPE)'''\n",
    "    y_error = y_true - y_pred\n",
    "    y_error_abs = np.array([abs(x) for x in y_error])\n",
    "    y_true_abs = np.array([abs(x) for x in y_true])\n",
    "    perc_error_abs = y_error_abs / y_true_abs\n",
    "    mape = perc_error_abs.sum() / len(y_true)\n",
    "    return mape\n",
    "\n",
    "def make_prediction(m, x_train, y_train, x_test, y_test):\n",
    "    m.fit(x_train, y_train)\n",
    "    y_pred = m.predict(x_test)\n",
    "    print('MAE:{:.2f} MSE:{:.2f} MAPE:{:.2f} R2:{:.2%}'\n",
    "          .format(mean_absolute_error(y_test, y_pred),\n",
    "                    mean_squared_error(y_test, y_pred),\n",
    "                    mape(y_test, y_pred),\n",
    "                    r2_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493d88a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the models and make predictionsRandomForestRegress\n",
    "for i in models:\n",
    "    print(i)\n",
    "    make_prediction(i, x_train_st, y_train, x_test_st, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4165403",
   "metadata": {},
   "source": [
    "**Conclusion:** 'Random Forest Regressor' and 'Gradient Boosting Regressor' obtained better results. Both will be compared with the gradient boosting models in the next section.\n",
    "\n",
    "**Remark:** The column names and feature importances coefficients (for Random Forest Regressor)\n",
    "are displayed to show which features most impact the algorithm's verdict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe02958b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show features\n",
    "print(x.columns)\n",
    "\n",
    "# Feature weights\n",
    "print(models[3].feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8e4326",
   "metadata": {},
   "source": [
    "Definitively, 'Weight' (feature importances coefficient = 0.5) is the most important factor that impacts the fuel consumption."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3824478",
   "metadata": {},
   "source": [
    "## Models with Gradient Boosting that do not belong to Scikit-Learn library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70254d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read DataFrame\n",
    "df = pd.read_csv('../data/processed/auto_cons_us_processed.csv', sep=',', header=0)\n",
    "\n",
    "# Show DataFrame\n",
    "print(df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b730922",
   "metadata": {},
   "source": [
    "Ensemble models like RandomForestRegressor and XGBRegressor work better with label encoding, so label encoding will be implemented in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d628558f",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoders = {}\n",
    "\n",
    "# Apply LabelEncoder to categorical columns\n",
    "for col in df.select_dtypes(include=['object']).columns:\n",
    "    encoder = LabelEncoder()\n",
    "    df[col] = encoder.fit_transform(df[col])\n",
    "    label_encoders[col] = encoder\n",
    "\n",
    "# Save label encoders\n",
    "joblib.dump(label_encoders, '../models/rg_label_enc.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8006e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain characteristic matrix (x) and objective variable (y)\n",
    "x = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "x, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Train the StandardScaler by using 'x_train', then transform 'x_train' and 'x_test'\n",
    "scaler = StandardScaler()\n",
    "x_train_st = scaler.fit_transform(x_train)\n",
    "x_test_st = scaler.transform(x_test)\n",
    "\n",
    "# Save scaler\n",
    "joblib.dump(scaler, '../models/rg_scaler.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the regression models\n",
    "models = [RandomForestRegressor(criterion='squared_error',\n",
    "                                max_depth=30,n_estimators=100, random_state=0),\n",
    "          GradientBoostingRegressor(loss='squared_error', random_state=0),\n",
    "          XGBRegressor(objective='reg:squarederror',\n",
    "                       n_estimators=100, learning_rate=0.1, random_state=0),\n",
    "          LGBMRegressor(objective='regression', metric='rmse', verbose=0,\n",
    "                        n_estimators=10, learning_rate=0.1, random_state=0),\n",
    "          CatBoostRegressor(loss_function=\"RMSE\", iterations=50, verbose=10,\n",
    "                            depth=10, cat_features=None, random_state=0),\n",
    "          ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the models and make predictionsRandomForestRegress\n",
    "for i in models:\n",
    "    print(i)\n",
    "    make_prediction(i, x_train_st, y_train, x_test_st, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cda28f9",
   "metadata": {},
   "source": [
    "GradientBoostingRegressor obtained better performance, it will be used to make predictions in production in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "joblib.dump(models[1], '../models/rg_GradientBoosting_model.joblib')\n",
    "print(\"Model saved successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env (3.8.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
